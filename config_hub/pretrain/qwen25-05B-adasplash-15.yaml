# Configuration for continuous pretraining of Qwen2.5-0.5B with AdaSplash attention

compiler: torch  # or thunder (not working)

# The name of the model to pretrain. (type: Optional[str], default: null)
model_name: Qwen2.5-0.5B

# A ``litgpt.Config`` object to define the model architecture. Mutually exclusive with
# ``model_config``. (type: Optional[Config], default: null)
model_config:

# Directory in which to save checkpoints and logs. (type: <class 'Path'>, default: out/pretrain)
out_dir: /mnt/scratch-artemis/mtreviso/ligtgpt/pretrain/

# The precision to use for pretraining. Possible choices: "bf16-true", "bf16-mixed", "32-true". (type: Optional[str], default: null)
precision: bf16-mixed

# Optional path to a checkpoint directory to initialize the model from.
# Useful for continued pretraining. Mutually exclusive with ``resume``. (type: Optional[Path], default: null)
initial_checkpoint_dir:

# Path to a checkpoint directory to resume from in case training was interrupted, or ``True`` to resume
# from the latest checkpoint in ``out_dir``. An error will be raised if no checkpoint is found. Passing
# ``'auto'`` will resume from the latest checkpoint but not error if no checkpoint exists.
# (type: Union[bool, Literal["auto"], Path], default: False)
resume: false

# Data-related arguments
data:
  class_path: litgpt.data.fineweb_edu.FinewebEdu
  init_args:
    dataset_name: HuggingFaceFW/fineweb-edu
    subset: sample-10BT
    split: train
    val_split_fraction: 0.001
    add_eos_token: true
    seed: 42
    num_workers: 4
    streaming: true
    max_dataset_length: 9672101
    preprocess_fn: litgpt.data.fineweb_edu.fineweb_preprocessing
    use_sample_packing: false
    packing_efficiency_factor: 1.2
    max_samples_to_pack: 8

# Training-related arguments
train:
  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)
  save_interval: 10000

  # Number of iterations between logging calls (type: int, default: 1)
  log_interval: 100

  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 128)
  global_batch_size: 16  # aka gradient_accumulation_steps

  # Number of samples per data-parallel rank (type: int, default: 4)
  micro_batch_size: 2

  # Number of iterations with learning rate warmup active (type: int, default: 100)
  lr_warmup_steps: 1000

  # Number of epochs to train on (type: Optional[int], default: null)
  epochs:

  # Total number of tokens to train on (type: Optional[int], default: null)
  max_tokens: 10000000000

  # Limits the number of optimizer steps to run (type: Optional[int], default: null)
  max_steps:

  # Limits the length of samples (type: Optional[int], default: null)
  max_seq_length: 4096

  # Whether to tie the embedding weights with the language modeling head weights (type: Optional[bool], default: null)
  tie_embeddings: true

  #   (type: Optional[float], default: 1.0)
  max_norm: 1.0

  # Learning rate parameter (type: float, default: 0.0003)
  min_lr: 0.0001

# Evaluation-related arguments
eval:
  # Number of optimizer steps between evaluation calls (type: int, default: 100)
  interval: 10000

  # Number of tokens to generate (type: Optional[int], default: null)
  max_new_tokens:

  # Number of iterations (type: int, default: 100)
  max_iters: 100

  # Whether to evaluate on the validation set at the beginning of the training
  initial_validation: false

  # Whether to evaluate on the validation set at the end the training
  final_validation: false

# Optimizer-related arguments
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    #   (type: float, default: 0.001)
    lr: 6e-4
    #   (type: float, default: 0.01)
    weight_decay: 0.1
    #   (type: tuple, default: (0.9,0.999))
    betas:
      - 0.9
      - 0.95

# How many devices/GPUs to use. (type: Union[int, str], default: auto)
devices: auto

# How many nodes to use. (type: int, default: 1)
num_nodes: 1

# Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data
# module require this. (type: Optional[Path], default: null)
tokenizer_dir: /mnt/scratch-artemis/mtreviso/models/Qwen2.5-0.5B

# The name of the logger to send metrics to. (type: Literal['wandb', 'tensorboard', 'csv', 'comet'], default: csv)
logger_name: comet

# The random seed to use for reproducibility. (type: int, default: 42)
seed: 42

# Custom callbacks configuration
#callbacks:
#  # Alpha scheduler for AdaSplash
#  alpha_scheduler:
#    class_path: alpha_scheduler.AlphaSchedulerCallback
#    init_args:
#      initial_alpha: 1.01
#      final_alpha: 1.5
#      strategy: linear
#      max_steps: null  # Will be set based on max_tokens
#
#  # Token monitoring callback
#  token_monitor:
#    class_path: token_monitor.TokenMonitorCallback
#    init_args:
#      target_tokens: 10000000000
#      log_interval: 100